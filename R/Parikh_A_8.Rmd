---
title: "DA5020 - Week 8 Assignment Web Scraping Programaically"
output:
  word_document: default
  pdf_document: default
date: '`r Sys.Date()`'
geometry:
  - margin=0.7in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  # mute messages output
  message = FALSE
)
```


In this week's assignment, we continue our pursuit for good burgers in specific neighborhoods in the Boston area using data from the yelp website. We will programatically extract specific fields in the data using the rvest package. This assignment will also provide practice in writing  functions and loops. Some questions require you to complete code within a partially written function given a specification. Other questions will not provide starter code.


## Questions

1. (20 points) Retrieve the contents of the first webpage for the yelp search as specified in Assignment 7 and write R statements to answer the following questions on the retrieved contents:

- How many nodes are direct descendents of the HTML `<body>` element (the actual visible content of a web page)?
A) 29 nodes are direct descendents of the HTML `<body>` element.
```{r}
library(rvest)
library(dplyr)
library(stringr)

page <- read_html("https://www.yelp.com/search?find_desc=Burgers&start=0&l=p:MA:Boston::%5BAllston/Brighton,Back_Bay,Beacon_Hill,Downtown,Fenway,South_End,West_End%5D")

# list the children of the <html> element (the whole page)
html_children(page)

# get the root of the actual html body
root <- html_node(page, 'body')

#extract html children from root
html_children(root) 
```


- What are the nodes names of the direct descendents of the `<body>`?
A) "script", "noscript" & "div" are the nodes names of the direct descendents of the `<body>` 
```{r}
names <- html_children(root) %>%
  html_name() 
unique(names)
```

- How many of these direct descendents have an `id` attribute?
A) Four of these direct descendents have an `id` attribute
```{r}
id <- html_children(root) %>%
  html_attr("id")
id
```


- What is the css selector to select restaurants that are advertisements? (You may not see the ads if you are logged in on Yelp or have an ad blocker running.)
A) ".yloca-tip" is the css selector to select restaurants that are advertisements.  

```{r}
ads <- page %>%
  html_nodes(css = ".yloca-tip")
ads
```

2. (50 points) Modify following parameterized function `get_yelp_sr_one_page` to extract a list of businesses on Yelp, for a specific search keyword, a specific location and a specific page of results.
```{r}
# I did not use the example code provided as it did not work properly for me
# I could not extract the addresses using the example no matter which method I used
# After trying 5 different ways, I finally gave up and did it like below
get_yelp_sr_one_page <- function(key,loc=NA,page=1){
  #function for creating URLs
  makeURL <- function(key,loc=NA,page=1){
    pg <- paste("https://www.yelp.com/search?find_desc=",key,sep="")

        ST <- str_extract(loc,",?([A-Z]{2})") #Extract State abbrev if included
        loc <- gsub("(,?\\s?[A-Z]{2})","",loc) #Remove State Abbrev
        loc <- gsub("\\s","+",loc) #format spaces appropriately
        if(is.na(ST)==F & is.character(ST)==T){ST <- paste(ST,":",sep="")
        loc <- paste("&find_loc=",ST,loc,sep="")
        } #Add : to ST abbrev
        pg <- paste(pg,loc,sep="")

    if(page>1){page <- (page-1)*10
    page <- paste("&start=",page,sep="")
    pg <- paste(pg,page,sep="")
    }
    return(pg)
  }
  URL <- makeURL(key,loc,page=1)#Make the URL
  
  #Get Results
  h <- read_html(URL)
  li <- html_nodes(h,css=".regular-search-result")
  #Extract parameters
  Name <- html_text(html_nodes(li,css=".biz-name"))
  URL <- html_attr(html_nodes(li,css=".biz-name"),"href")
  Price <- nchar(html_text(html_nodes(li,css=".price-range")),type="chars")
  Ser_Cat <- gsub("\\s{2,}","",html_text(html_nodes(li,css=".category-str-list")))
  Telephone <- gsub("\\s{2,}","",html_text(html_nodes(li,css=".biz-phone")))
  NH <- gsub("\\s{2,}","",html_text(html_nodes(li,css=".neighborhood-str-list")))
  add <- html_nodes(li,css="address")
  Street <- gsub("\\s{2,}","",str_extract(add,"(?<=\\n)[A-Za-z0-9\\s]+(?=<br>)"))
  City <- gsub("\\s{2,}","",str_extract(add,"(?<=<br>)[A-Za-z0-9\\s]+(?=,)"))
  State <- str_extract(add,"[A-Z]{2}")
  Zip <- str_extract(add,"[0-9]{5}")
  Avg_rat <- str_extract(html_attr(html_nodes(li,css=".i-stars"),"title"),"\\d.\\d")
  Num_rew <- str_extract(html_text(html_nodes(li,css=".review-count")),"\\d+")
  Rev_URL <- html_attr(html_nodes(li,css="p.snippet a.nowrap"),"href")
  #Create a list of values for error checking
  cols <- list(Name=Name,URL=URL,Price=Price,Ser_Cat=Ser_Cat,Telephone=Telephone,NH=NH,Street=Street,City=City,State=State,Zip=Zip,Avg_rat=Avg_rat,Num_rew=Num_rew,Rev_URL=Rev_URL)
  #Test for missing results, if missing use the for loop provided to extract each value individually and add NA for missing values.
  do.Index <- vector("character")
  for(i in seq_along(cols)){
    if (length(cols[[i]]) < length(cols[[1]])) {
      do.Index <- (names(cols)[[i]])
    }
  }
  if("NH" %in% do.Index==T){
    NH <- vector("character")
    for(i in seq_along(li)){
      node <- ifelse(
        is.null(html_node(li[[i]], css=".neighborhood-str-list")), 
        NA,
        html_node(li[[i]], css=".neighborhood-str-list")  %>% 
          html_text()
      )
      NH <- append(NH,node,after=length(NH))
      NH <- gsub("\\s{2,}","",NH)
    }
  } 
  if("Telephone" %in% do.Index==T){
    Telephone <- vector("character")
    for(i in seq_along(li)){
      node <- ifelse(
        is.null(html_nodes(li,css=".biz-phone")), 
        NA,
        html_nodes(li,css=".biz-phone")  %>% 
          html_text()
      )
      Telephone <- append(Telephone,node,after=length(NH))
      Telephone <- gsub("\\s{2,}","",Telephone)
    }
  }
  if("Price" %in% do.Index==T){
    Price <- vector("character")
    for(i in seq_along(li)){
      Price[i] <- ifelse(
        is.null(html_nodes(li[i],css=".price-range")), 
        NA,
        html_nodes(li[i],css=".price-range")  %>% html_text() %>% 
          nchar("chars")
      )
      Price <- gsub("\\s{2,}","",Price)
    }
  }
  if("Street" %in% do.Index==T){
    Street <- vector("character")
    for(i in seq_along(li)){
      node <- ifelse(
        is.null(html_nodes(li,css="address")), 
        NA,
        html_nodes(li,css="address")  %>% 
          str_extract("(?<=\\n)[A-Za-z0-9\\s]+(?=<br>)")
      )
      Street <- append(Street,node,after=length(Street))
      Street <- gsub("\\s{2,}","",Street)
    }
  }
  if("City" %in% do.Index==T){
    City <- vector("character")
    for(i in seq_along(li)){
      node <- ifelse(
        is.null(html_nodes(li,css="address")), 
        NA,
        html_nodes(li,css="address")  %>% 
          str_extract("(?<=<br>)[A-Za-z0-9\\s]+(?=,)")
      )
      City <- append(City,node,after=length(City))
      City <- gsub("\\s{2,}","",City)
    }
  }
  if("State" %in% do.Index==T){
    State <- vector("character")
    for(i in seq_along(li)){
      node <- ifelse(
        is.null(html_nodes(li,css="address")), 
        NA,
        html_nodes(li,css="address")  %>% 
          str_extract("[A-Z]{2}")
      )
      State <- append(State,node,after=length(State))
    }
  }
  if("Zip" %in% do.Index==T){
    Zip <- vector("character")
    for(i in seq_along(li)){
      node <- ifelse(
        is.null(html_nodes(li,css="address")), 
        NA,
        html_nodes(li,css="address")  %>% 
          str_extract("[0-9]{5}")
      )
      Zip <- append(Zip,node,after=length(Zip))
    }
  }
  
  pg <- cbind(Name,URL,Price,Ser_Cat,Telephone,NH,Street,City,State,Zip,Avg_rat,Num_rew,Rev_URL)
  #Create the output matrix (must be a matrix for the 2nd fn to work)
  return(pg)
}
result <- get_yelp_sr_one_page("burger",loc="Boston,MA",page=1)
head(result)
```

3. (20 points) Write a function that reads multiple pages of the search results of any search keyword and location from Yelp.

Note that for some queries, Yelp may get a different number of results per page. You would need to either change the way you calculate the URL parameter, or use the `distinct(df)` function to remove duplicate rows.
```{r}
mult_pages <- function(key,loc,pages){
  mat <- matrix(ncol=13,nrow=0)
  for(i in seq_along(pages)){
  pg <- get_yelp_sr_one_page(key,loc,page=i)
  mat <- rbind(mat,pg)

  }
  df <- as.data.frame(mat,stringsAsFactors=F)
  return(df)
}
result1 <- mult_pages("Vegetarian","Boston,MA",1:5)
head(result1)
```


4. (10 points) Optimize your function in question 3, add a small wait time (0.5s for example) between each request, so that you don't get banned by Yelp for abusing their website (hint: use `Sys.sleep()`).

```{r}
mult_pages <- function(key,loc,pages){
  mat <- matrix(ncol=13,nrow=0)
  for(i in seq_along(pages)){
  pg <- get_yelp_sr_one_page(key,loc,page=i)
  mat <- rbind(mat,pg)
  Sys.sleep(0.5)
  }
  df <- as.data.frame(mat,stringsAsFactors=F)
  return(df)
}
result1 <- mult_pages("Vegetarian","Boston,MA",1:5)
head(result1)
```
## Submission
You need to submit an .Rmd extension file as well as the generated pdf file. Be sure to state all the assumptions and give explanations as comments in the .Rmd file wherever needed to help us assess your submission. Remember to use the naming convention you have been following in this course so far. 
